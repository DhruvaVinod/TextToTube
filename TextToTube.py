import cv2
import easyocr
import json
import numpy as np
import os
import requests
import webbrowser
import yt_dlp
import googleapiclient.discovery
import re
import tempfile
import pygame

from gtts import gTTS
from faster_whisper import WhisperModel
from sentence_transformers import SentenceTransformer
import gradio as gr
import time
from deep_translator import GoogleTranslator, exceptions as dt_exceptions

from PIL import Image, ImageDraw, ImageFont
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from matplotlib.animation import FuncAnimation
import io
import base64
import subprocess 

pygame.mixer.init()

# API Keys
YOUTUBE_API_KEY = "AIzaSyD6hKgUxy-91DW8AnaTrc7nvDHUfWazi_0"
GEMINI_API_KEY = "AIzaSyDDwEucj4KNsnUT4m4qpt1pwnByhm6_vjM"

#Language Code Mapping
LANGUAGE_CODE_MAP = {
    "Hindi": "hi",
    "Tamil": "ta",
    "Telugu": "te",
    "Kannada": "kn",
    "Malayalam": "ml",
    "Bengali": "bn",
    "Gujarati": "gu",
    "Marathi": "mr",
}

# Global variable to store current audio file path
current_audio_path = None

# Global variables for quiz
current_quiz_data = None
user_answers = {}

#cleaning the text generated by gemini
def clean_text(text):
    # Remove Markdown-style headers, bold, bullets, and excess whitespace
    text = re.sub(r"[*#‚Ä¢]+", "", text)  # remove *, #, ‚Ä¢
    text = re.sub(r"\n{2,}", "\n", text)  # reduce multiple newlines to one
    text = re.sub(r"\s{2,}", " ", text)  # collapse multiple spaces
    return text.strip()

# Utility Functions
def scan_headline():
    cap = cv2.VideoCapture(0)
    while True:
        ret, frame = cap.read()
        if not ret or frame is None:
            break
        cv2.imwrite("captured_frame.jpg", frame)
        break
    cap.release()
    cv2.destroyAllWindows()
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    reader = easyocr.Reader(['en'])
    results = reader.readtext(gray)
    if not results:
        return ""
    return " ".join([res[1] for res in results])

def extract_text_from_image(image_path):
    """Extract text from uploaded image file using OCR"""
    try:
        # Read the image
        image = cv2.imread(image_path)
        if image is None:
            return "‚ùå Could not read the image file. Please check the file format."
        
        # Convert to grayscale for better OCR results
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        
        # Initialize EasyOCR reader
        reader = easyocr.Reader(['en'])
        
        # Perform OCR
        results = reader.readtext(gray)
        
        if not results:
            return "‚ùå No text found in the image."
        
        # Extract and join all detected text
        extracted_text = " ".join([res[1] for res in results])
        return extracted_text
        
    except Exception as e:
        return f"‚ùå Error processing image: {str(e)}"

def get_video(query):
    model = SentenceTransformer("all-MiniLM-L6-v2")
    youtube = googleapiclient.discovery.build("youtube", "v3", developerKey=YOUTUBE_API_KEY)
    search_response = youtube.search().list(q=query, part="snippet", maxResults=10, type="video").execute()

    videos = [{
        "title": item["snippet"]["title"],
        "description": item["snippet"]["description"],
        "url": f"https://www.youtube.com/watch?v={item['id']['videoId']}"
    } for item in search_response.get("items", [])]

    if not videos:
        return None

    query_emb = model.encode(query, convert_to_tensor=True).cpu().numpy()

    similarities = []
    for video in videos:
        title_emb = model.encode(video["title"], convert_to_tensor=True).cpu().numpy()
        desc_emb = model.encode(video["description"], convert_to_tensor=True).cpu().numpy()
        sim = (np.dot(query_emb, title_emb) + np.dot(query_emb, desc_emb)) / (
            np.linalg.norm(query_emb) * (np.linalg.norm(title_emb) + np.linalg.norm(desc_emb))
        )
        similarities.append(sim)

    best_index = np.argmax(similarities)
    return videos[best_index]["title"], videos[best_index]["url"]

def generate_notes_with_gemini(text):
    prompt = f"Generate study notes from the given topic:\n\n{text}"
    data = {"contents": [{"parts": [{"text": prompt}]}]}
    headers = {'Content-Type': 'application/json'}
    url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={GEMINI_API_KEY}"
    response = requests.post(url, headers=headers, data=json.dumps(data))
    if response.status_code == 200:
        raw_notes = response.json()["candidates"][0]["content"]["parts"][0]["text"]
        return clean_text(raw_notes)
    else:
        return "‚ùå Error generating notes."

def generate_summary_with_gemini(text: str):
        API_KEY = GEMINI_API_KEY
        url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={API_KEY}"
        prompt = f"Summarize the following text in detail as refernce notes for a student:\n{text}"
        data = {"contents": [{"parts": [{"text": prompt}]}]}
        headers = {'Content-Type': 'application/json'}
        response = requests.post(url, headers=headers, data=json.dumps(data))
        if response.status_code == 200:
            summary = response.json()["candidates"][0]["content"]["parts"][0]["text"]
            return clean_text(summary)
        else:
            return "Error generating summary"

def summarize_video(text):
    result = get_video(text)
    if result is None:
        return "No video found"
    _, video_url = result
    audio_path = download_audio(video_url)
    raw_transcript = transcribe_audio(audio_path)
    return generate_summary_with_gemini(raw_transcript)  

def download_audio(video_url):
    ffmpeg_path = "C:/ffmpeg/ffmpeg-7.1.1-essentials_build/bin"
    ydl_opts = {
        'ffmpeg_location': ffmpeg_path,
        'format': 'bestaudio/best',
        'postprocessors': [{
            'key': 'FFmpegExtractAudio',
            'preferredcodec': 'mp3',
            'preferredquality': '192'
        }],
        'no_part': True,
        'force_overwrites': True
    }
    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        info = ydl.extract_info(video_url, download=True)
        return ydl.prepare_filename(info).rsplit(".", 1)[0] + ".mp3"

def transcribe_audio(audio_path):
    model = WhisperModel("base", device="cpu", compute_type="int8")
    #model = WhisperModel("base")
    segments, _ = model.transcribe(audio_path)
    return " ".join([segment.text for segment in segments])

def translate_text(text, lang_name, retries=3):
    lang_code = LANGUAGE_CODE_MAP.get(lang_name, "hi")
    max_chunk_size = 5000

    def split_text(text, size):
        return [text[i:i+size] for i in range(0, len(text), size)]

    chunks = split_text(text, max_chunk_size)
    translated = []

    for chunk in chunks:
        for attempt in range(retries):
            try:
                translated_chunk = GoogleTranslator(source="auto", target=lang_code).translate(chunk)
                translated.append(translated_chunk)
                break  # Success
            except dt_exceptions.RequestError:
                if attempt < retries - 1:
                    time.sleep(1)  # wait and retry
                else:
                    return f"‚ùå Failed to translate after {retries} attempts. Check your internet connection."
            except Exception as e:
                return f"‚ùå Unexpected error: {e}"
    return clean_text("\n".join(translated))

def generate_audio(text, lang_name):
    """Generate audio file and return the file path"""
    global current_audio_path
    
    if not text or not text.strip():
        return None, "‚ùå No text provided for audio generation"
    
    lang_code = LANGUAGE_CODE_MAP.get(lang_name, "en")
    
    try:
        # Create a temporary file
        with tempfile.NamedTemporaryFile(suffix=".mp3", delete=False) as temp_file:
            audio_path = temp_file.name
        
        tts = gTTS(text=text, lang=lang_code)
        tts.save(audio_path)
        current_audio_path = audio_path
        return audio_path, "‚úÖ Audio generated successfully!"
        
    except Exception as e:
        return None, f"‚ùå Audio generation error: {str(e)}"

def play_audio():
    """Play audio with pygame"""
    global current_audio_path
    try:
        if current_audio_path and os.path.exists(current_audio_path):
            if pygame.mixer.music.get_busy():
                pygame.mixer.music.stop()
            pygame.mixer.music.load(current_audio_path)
            pygame.mixer.music.play()
            return "üîä Audio playing..."
        else:
            return "‚ùå No audio file available to play"
    except Exception as e:
        return f"‚ùå Playback error: {str(e)}"

def pause_audio():
    """Pause audio playback"""
    try:
        if pygame.mixer.music.get_busy():
            pygame.mixer.music.pause()
            return "‚è∏Ô∏è Audio paused"
        else:
            return "‚ùå No audio currently playing"
    except Exception as e:
        return f"‚ùå Pause error: {str(e)}"

def resume_audio():
    """Resume audio playback"""
    try:
        pygame.mixer.music.unpause()
        return "‚ñ∂Ô∏è Audio resumed"
    except Exception as e:
        return f"‚ùå Resume error: {str(e)}"

def stop_audio():
    """Stop audio playback"""
    try:
        pygame.mixer.music.stop()
        return "‚èπÔ∏è Audio stopped"
    except Exception as e:
        return f"‚ùå Stop error: {str(e)}"

def get_text_for_audio(english_text, translated_text, language):
    """Determine which text to use for audio generation"""
    if language == "English" or not translated_text or translated_text.strip() == "":
        return english_text
    return translated_text

# MODIFIED QUIZ GENERATION FUNCTIONS
def generate_quiz_questions_with_gemini(text, num_questions=10):
    """Generate quiz questions using Gemini API"""
    prompt = f"""Generate {num_questions} multiple choice questions based on the following topic: {text}

For each question, provide:
1. The question text
2. Four answer options (A, B, C, D)
3. The correct answer (A, B, C, or D)
4. A brief explanation of why the answer is correct

Format the output as JSON with the following structure:
{{
  "questions": [
    {{
      "question": "Question text here?",
      "options": {{
        "A": "Option A text",
        "B": "Option B text", 
        "C": "Option C text",
        "D": "Option D text"
      }},
      "correct_answer": "A",
      "explanation": "Explanation text here"
    }}
  ]
}}

Make sure the questions are educational and test understanding of the key concepts."""

    data = {"contents": [{"parts": [{"text": prompt}]}]}
    headers = {'Content-Type': 'application/json'}
    url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={GEMINI_API_KEY}"
    
    try:
        response = requests.post(url, headers=headers, data=json.dumps(data))
        if response.status_code == 200:
            raw_response = response.json()["candidates"][0]["content"]["parts"][0]["text"]
            
            # Clean the response to extract JSON
            json_start = raw_response.find('{')
            json_end = raw_response.rfind('}') + 1
            
            if json_start != -1 and json_end != -1:
                json_str = raw_response[json_start:json_end]
                quiz_data = json.loads(json_str)
                return quiz_data
            else:
                return {"error": "Failed to parse quiz questions from response"}
        else:
            return {"error": f"API request failed with status {response.status_code}"}
    except json.JSONDecodeError:
        return {"error": "Failed to parse JSON response from Gemini"}
    except Exception as e:
        return {"error": f"Error generating quiz: {str(e)}"}

def generate_quiz_interface(topic, num_questions):
    """Generate quiz and return Gradio components"""
    global current_quiz_data, user_answers
    
    if not topic or not topic.strip():
        return [gr.update(visible=False)] * 20  # Hide all quiz components
    
    try:
        num_q = int(num_questions) if num_questions else 10
        quiz_data = generate_quiz_questions_with_gemini(topic, num_q)
        
        if "error" in quiz_data:
            return [gr.update(visible=False)] * 20  # Hide all components on error
        
        current_quiz_data = quiz_data
        user_answers = {}
        
        questions = quiz_data.get("questions", [])
        updates = []
        
        # Generate updates for each question component (up to 10 questions)
        for i in range(10):
            if i < len(questions):
                q = questions[i]
                question_text = f"**Question {i+1}:** {q['question']}"
                options = [f"{key}. {value}" for key, value in q['options'].items()]
                
                updates.extend([
                    gr.update(value=question_text, visible=True),  # Question text
                    gr.update(choices=options, value=None, visible=True)  # Radio options
                ])
            else:
                updates.extend([
                    gr.update(visible=False),  # Hide question
                    gr.update(visible=False)   # Hide options
                ])
        
        return updates
        
    except Exception as e:
        return [gr.update(visible=False)] * 20  # Hide all components on error

def submit_quiz(*answers):
    """Submit quiz and calculate score"""
    global current_quiz_data, user_answers
    
    if not current_quiz_data:
        return "‚ùå No quiz data available"
    
    questions = current_quiz_data.get("questions", [])
    score = 0
    total = len(questions)
    results = []
    
    for i, answer in enumerate(answers[:total]):
        if i < len(questions):
            q = questions[i]
            correct_answer = q['correct_answer']
            
            # Extract selected option letter (A, B, C, D)
            selected_option = None
            if answer:
                selected_option = answer[0]  # First character should be the option letter
            
            is_correct = selected_option == correct_answer
            if is_correct:
                score += 1
            
            results.append(f"""
**Question {i+1}:** {q['question']}
**Your Answer:** {answer if answer else 'Not answered'}
**Correct Answer:** {correct_answer}. {q['options'][correct_answer]}
**Explanation:** {q['explanation']}
**Status:** {'‚úÖ Correct' if is_correct else '‚ùå Incorrect'}
""")
    
    result_text = f"""
# üéØ Quiz Results

**Score: {score}/{total} ({(score/total)*100:.1f}%)**

---

{''.join(results)}
"""
    
    return result_text

# UI Functions
def ui_scan_headline():
    """Original webcam scanning function"""
    #return scan_headline()
    return "Computer Networks"

def ui_upload_and_extract(uploaded_file):
    """Handle uploaded file and extract text"""
    if uploaded_file is None:
        return "‚ùå No file uploaded. Please upload an image file."
    
    try:
        # Get the file path from the uploaded file
        file_path = uploaded_file.name
        
        # Extract text from the uploaded image
        extracted_text = extract_text_from_image(file_path)
        
        return extracted_text
        
    except Exception as e:
        return f"‚ùå Error processing uploaded file: {str(e)}"

def ui_watch_video(text):
    result = get_video(text)
    if result is None:
        return "No video found", ""
    title, url = result
    webbrowser.open(url)
    return title, url

def ui_summarize_video(text):
    return summarize_video(text)

def ui_translate_summary(english_text, target_language):
    return translate_text(english_text, target_language)

def ui_generate_audio_summary(english_text, translated_text, language):
    """Generate audio for summary"""
    text_to_speak = get_text_for_audio(english_text, translated_text, language)
    if not text_to_speak:
        return "‚ùå No text available to generate audio"
    
    audio_path, message = generate_audio(text_to_speak, language)
    return message

def ui_generate_audio_notes(english_text, translated_text, language):
    """Generate audio for notes"""
    text_to_speak = get_text_for_audio(english_text, translated_text, language)
    if not text_to_speak:
        return "‚ùå No text available to generate audio"
    
    audio_path, message = generate_audio(text_to_speak, language)
    return message

def conditional_translate_summary(text, lang):
    return text if lang == "English" else translate_text(text, lang)

def conditional_translate_notes(text, lang):
    return text if lang == "English" else translate_text(text, lang)

# File saving functions
def save_text_to_file(text, filename):
    if text:
        with open(filename, "w", encoding="utf-8") as f:
            f.write(text)
        return filename
    return None

# Gradio UI
custom_css = """
body {
    background-image: url('webbg.png'); 
    background-size: cover;
    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
}
h1 {
    text-align: center; 
    color: #000000; 
    background-color: #FFFDD0; 
    padding: 20px;
    border-radius: 15px;
    font-size: 2.2rem;
}
.gr-button {
    font-size: 16px;
    font-weight: 600;
    background-color: #add8e6 !important;
    color: white !important;
    border: none !important;
    border-radius: 8px !important;
    padding: 10px 20px;
}
.or-divider {
    text-align: center;
    font-size: 18px;
    font-weight: 600;
    color: #666;
    margin: 15px 0;
    position: relative;
}
.or-divider::before,
.or-divider::after {
    content: '';
    position: absolute;
    top: 50%;
    width: 45%;
    height: 1px;
    background-color: #ccc;
}
.or-divider::before {
    left: 0;
}
.or-divider::after {
    right: 0;
}
"""

def generate_video_script_with_gemini(topic_text):
    """Generate a detailed video script with visual cues using Gemini"""
    prompt = f"""Create a detailed educational video script for the topic: {topic_text}

The script should include:
1. An engaging introduction (30 seconds)
2. Main content broken into 3-4 key sections (2-3 minutes each)
3. A conclusion with key takeaways (30 seconds)
4. Visual descriptions for each section (what should be shown on screen)
5. Total duration should be 8-10 minutes

Format the output as JSON:
{{
  "title": "Video Title",
  "total_duration": "10 minutes",
  "sections": [
    {{
      "section_title": "Introduction",
      "duration": "30 seconds",
      "script": "Spoken content here...",
      "visual_description": "What should be shown on screen",
      "key_points": ["point 1", "point 2"]
    }}
  ]
}}

Make the language simple, engaging, and educational. Include real-world examples and analogies."""

    data = {"contents": [{"parts": [{"text": prompt}]}]}
    headers = {'Content-Type': 'application/json'}
    url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={GEMINI_API_KEY}"
    
    try:
        response = requests.post(url, headers=headers, data=json.dumps(data))
        if response.status_code == 200:
            raw_response = response.json()["candidates"][0]["content"]["parts"][0]["text"]
            
            # Clean the response to extract JSON
            json_start = raw_response.find('{')
            json_end = raw_response.rfind('}') + 1
            
            if json_start != -1 and json_end != -1:
                json_str = raw_response[json_start:json_end]
                script_data = json.loads(json_str)
                return script_data
            else:
                return {"error": "Failed to parse video script from response"}
        else:
            return {"error": f"API request failed with status {response.status_code}"}
    except json.JSONDecodeError:
        return {"error": "Failed to parse JSON response from Gemini"}
    except Exception as e:
        return {"error": f"Error generating video script: {str(e)}"}

def create_slide_with_opencv(text, section_title, slide_number, total_slides, width=1920, height=1080):
    """Create a visual slide using OpenCV and PIL with enhanced graphics"""
    try:
        # Create PIL image with gradient background
        img = Image.new('RGB', (width, height), color='#ffffff')
        draw = ImageDraw.Draw(img)
        
        # Create gradient background
        for y in range(height):
            # Blue gradient from light to darker
            r = int(240 - (y * 40 / height))
            g = int(248 - (y * 30 / height))
            b = int(255 - (y * 20 / height))
            color = (max(200, r), max(220, g), max(235, b))
            draw.line([(0, y), (width, y)], fill=color)
        
        # Add decorative elements
        # Header background rectangle
        header_height = 150
        draw.rectangle([0, 0, width, header_height], fill='#2c3e50', outline='#34495e', width=3)
        
        # Add geometric shapes for visual appeal
        # Left side decorative circles
        circle_colors = ['#3498db', '#e74c3c', '#f39c12', '#2ecc71']
        for i, color in enumerate(circle_colors):
            x = 50 + (i * 60)
            y = 50
            draw.ellipse([x, y, x+40, y+40], fill=color)
        
        # Right side decorative triangles
        for i in range(3):
            x = width - 200 + (i * 50)
            y = 30 + (i * 15)
            points = [(x, y), (x+30, y), (x+15, y+25)]
            draw.polygon(points, fill='#9b59b6')
        
        # Try to load fonts
        try:
            title_font = ImageFont.truetype("arial.ttf", 56)
            content_font = ImageFont.truetype("arial.ttf", 32)
            footer_font = ImageFont.truetype("arial.ttf", 28)
            bullet_font = ImageFont.truetype("arial.ttf", 30)
        except:
            try:
                # Try different font names
                title_font = ImageFont.truetype("Arial.ttf", 56)
                content_font = ImageFont.truetype("Arial.ttf", 32)
                footer_font = ImageFont.truetype("Arial.ttf", 28)
                bullet_font = ImageFont.truetype("Arial.ttf", 30)
            except:
                # Fallback to default fonts with larger size
                title_font = ImageFont.load_default()
                content_font = ImageFont.load_default()
                footer_font = ImageFont.load_default()
                bullet_font = ImageFont.load_default()
        
        # Draw title with shadow effect
        title_y = 75
        # Shadow
        draw.text((width//2 + 2, title_y + 2), section_title, font=title_font, 
                  fill='#1a252f', anchor='mt')
        # Main title
        draw.text((width//2, title_y), section_title, font=title_font, 
                  fill='#ffffff', anchor='mt')
        
        # Content area background
        content_bg_y = header_height + 20
        content_bg_height = height - content_bg_y - 100
        draw.rectangle([50, content_bg_y, width-50, content_bg_y + content_bg_height], 
                      fill='#ffffff', outline='#bdc3c7', width=2)
        
        # Add subtle pattern to content area
        for i in range(0, width-100, 100):
            for j in range(content_bg_y, content_bg_y + content_bg_height, 100):
                draw.ellipse([i+75, j+20, i+85, j+30], fill='#ecf0f1')
        
        # Process and draw content with bullet points
        words = text.split()
        lines = []
        current_line = []
        chars_per_line = 65
        
        # Create bullet points from sentences
        sentences = text.split('.')
        bullet_lines = []
        
        for sentence in sentences[:8]:  # Limit to 8 bullet points
            sentence = sentence.strip()
            if sentence and len(sentence) > 10:
                # Word wrap long sentences
                if len(sentence) > chars_per_line:
                    words_in_sentence = sentence.split()
                    wrapped_lines = []
                    current_wrap = []
                    
                    for word in words_in_sentence:
                        if len(' '.join(current_wrap + [word])) <= chars_per_line:
                            current_wrap.append(word)
                        else:
                            if current_wrap:
                                wrapped_lines.append(' '.join(current_wrap))
                            current_wrap = [word]
                    if current_wrap:
                        wrapped_lines.append(' '.join(current_wrap))
                    
                    bullet_lines.extend(wrapped_lines)
                else:
                    bullet_lines.append(sentence)
        
        # Draw bullet points
        content_start_y = content_bg_y + 40
        line_height = 55
        max_lines = 10
        
        for i, line in enumerate(bullet_lines[:max_lines]):
            if line.strip():
                y_pos = content_start_y + (i * line_height)
                
                # Draw bullet point
                bullet_x = 100
                bullet_y = y_pos + 15
                draw.ellipse([bullet_x, bullet_y, bullet_x+12, bullet_y+12], fill='#3498db')
                
                # Draw text
                text_x = bullet_x + 25
                draw.text((text_x, y_pos), line.strip(), font=content_font, fill='#2c3e50')
        
        # Footer section
        footer_y = height - 80
        draw.rectangle([0, footer_y, width, height], fill='#34495e')
        
        # Slide number with style
        footer_text = f"Slide {slide_number} of {total_slides}"
        draw.text((width-50, footer_y+25), footer_text, font=footer_font, 
                  fill='#ffffff', anchor='rt')
        
        # Add progress bar
        progress_width = 400
        progress_x = 50
        progress_y = footer_y + 30
        
        # Progress background
        draw.rectangle([progress_x, progress_y, progress_x + progress_width, progress_y + 8], 
                      fill='#7f8c8d')
        
        # Progress fill
        fill_width = int((slide_number / total_slides) * progress_width)
        draw.rectangle([progress_x, progress_y, progress_x + fill_width, progress_y + 8], 
                      fill='#3498db')
        
        # Add logo/branding area
        brand_text = "AI Learning Assistant"
        draw.text((50, footer_y+10), brand_text, font=footer_font, fill='#ecf0f1')
        
        # Convert PIL to OpenCV format
        opencv_image = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)
        
        return opencv_image
        
    except Exception as e:
        print(f"Error creating slide: {e}")
        # Return a colorful fallback frame
        fallback_img = np.zeros((height, width, 3), dtype=np.uint8)
        
        # Create gradient fallback
        for y in range(height):
            color_val = int(100 + (y * 155 / height))
            fallback_img[y, :] = [color_val, 200, 255]
        
        # Add simple text overlay using OpenCV
        cv2.putText(fallback_img, section_title, (50, 100), 
                   cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 3)
        cv2.putText(fallback_img, f"Slide {slide_number}/{total_slides}", 
                   (50, height-50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
        
        return fallback_img

def combine_audio_video_ffmpeg(video_path, audio_path, output_path):
    """Combine video and audio using ffmpeg subprocess"""
    try:
        ffmpeg_path = "C:/ffmpeg/ffmpeg-7.1.1-essentials_build/bin/ffmpeg.exe"
        
        # Check if custom ffmpeg exists, otherwise use system ffmpeg
        if not os.path.exists(ffmpeg_path):
            ffmpeg_path = "ffmpeg"
        
        cmd = [
            ffmpeg_path,
            '-i', video_path,
            '-i', audio_path,
            '-c:v', 'copy', #changed from 'libx264'
            '-c:a', 'copy', #changed from 'aac'
            '-map', '0:v:0', #added new 
            '-map', '1:a:0', #added new 
            #'-strict', 'experimental',
            '-shortest',
            '-y',  # Overwrite output file
            output_path
        ]
        
        result = subprocess.run(cmd, capture_output=True, text=True)
        
        if result.returncode == 0:
            return True, "Video and audio combined successfully"
        else:
            return False, f"FFmpeg error: {result.stderr}"
            
    except Exception as e:
        return False, f"Error combining audio and video: {str(e)}"

def generate_educational_video_opencv(topic_text):
    """Generate educational video using OpenCV instead of moviepy"""
    try:
        # Step 1: Generate script using Gemini
        script_data = generate_video_script_with_gemini(topic_text)
        
        if "error" in script_data:
            return f"‚ùå Error generating script: {script_data['error']}"
        
        # Step 2: Extract full script text for audio
        full_script = ""
        sections = script_data.get("sections", [])
        
        for section in sections:
            full_script += section.get("script", "") + " "
        
        if not full_script.strip():
            return "‚ùå No script content generated"
        
        # Step 3: Generate audio from script
        audio_path, audio_status = generate_audio(full_script, "English")
        
        if not audio_path:
            return f"‚ùå Audio generation failed: {audio_status}"
        
        # Step 4: Create video using OpenCV
        video_width = 1920
        video_height = 1080
        fps = 30
        
        # Estimate duration (rough: 150 words per minute)
        word_count = len(full_script.split())
        estimated_duration = max((word_count / 150) * 60, 30)  # At least 30 seconds
        
        # Calculate frames and duration per slide
        total_frames = int(estimated_duration * fps)
        frames_per_slide = max(total_frames // len(sections), fps * 5)  # At least 5 seconds per slide
        
        # Create video writer
        temp_video_path = "temp_educational_video.mp4"
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        video_writer = cv2.VideoWriter(temp_video_path, fourcc, fps, (video_width, video_height))
        
        if not video_writer.isOpened():
            return "‚ùå Error: Could not create video writer"
        
        # Generate frames for each section
        for i, section in enumerate(sections):
            slide_img = create_slide_with_opencv(
                section.get("script", "")[:800] + "...",  # Limit text length
                section.get("section_title", f"Section {i+1}"),
                i + 1,
                len(sections),
                video_width,
                video_height
            )
            
            # Write frames for this slide
            for frame_num in range(frames_per_slide):
                video_writer.write(slide_img)
        
        video_writer.release()
        
        # Step 5: Combine video with audio using ffmpeg
        final_output_path = "educational_video.mp4"
        success, message = combine_audio_video_ffmpeg(temp_video_path, audio_path, final_output_path)
        
        # Clean up temporary files
        if os.path.exists(temp_video_path):
            os.remove(temp_video_path)
        if os.path.exists(audio_path):
            os.remove(audio_path)
        
        if success:
            return f"‚úÖ Educational video generated successfully! Saved as: {final_output_path}"
        else:
            return f"‚ùå Error combining video and audio: {message}"
        
    except Exception as e:
        return f"‚ùå Error generating video: {str(e)}"

# UI function for the dropdown
def ui_generate_educational_video(topic_text):
    if not topic_text or not topic_text.strip():
        return "‚ùå Please provide a topic first", None, gr.update(visible=False)
    
    result = generate_educational_video_opencv(topic_text)
    
    if "‚úÖ" in result and "educational_video.mp4" in result:
        return result, "educational_video.mp4", gr.update(value="educational_video.mp4", visible=True)
    else:
        return result, None, gr.update(visible=False)

with gr.Blocks(css=custom_css) as demo:
    gr.Markdown("<h1>üì∞ Text to Learning Assistant</h1>")

    gr.Markdown("## üì∑ Text Input Options")
    
    # Webcam scan button on top
    btn_scan = gr.Button("üì∑ Scan with Webcam", variant="primary", size="lg")
    
    # OR divider
    gr.HTML('<div class="or-divider">OR</div>')
    
    # File upload section below
    file_upload = gr.File(
        label="", 
        file_types=["image"], 
        type="filepath",
        show_label=False
    )
    btn_extract = gr.Button("üîç Extract Text from Upload", variant="secondary")
    
    output_text = gr.Textbox(label="üìÑ Extracted Text", interactive=True, lines=3)

    gr.Markdown("## ‚û°Ô∏è What would you like to do?")

    with gr.Accordion("‚ñ∂Ô∏è Option 1: Find and Summarize YouTube Video", open=False):
        with gr.Row():
            btn_watch = gr.Button("üîç Find Best Video")
            btn_summary = gr.Button("üìÑ Generate Summary")
        video_title = gr.Textbox(label="üéûÔ∏è Video Title", interactive=False)
        video_url = gr.Textbox(label="üîó Video URL", interactive=False)

        english_summary = gr.Textbox(label="üìù English Summary", interactive=False)
        language_selector_summary = gr.Dropdown(
            choices=["English", "Hindi", "Tamil", "Telugu", "Kannada", "Malayalam", "Bengali", "Gujarati", "Marathi"],
            label="üåê Translate Summary To",
            value="English"
        )
        translated_summary = gr.Textbox(label="üåç Translated Summary", interactive=False)
        
        # Audio Controls for Summary
        with gr.Row():
            btn_generate_audio_summary = gr.Button("üîä Generate Audio")
            btn_play_summary = gr.Button("‚ñ∂Ô∏è Play")
            btn_pause_summary = gr.Button("‚è∏Ô∏è Pause")
            btn_resume_summary = gr.Button("üîÑ Resume")
            btn_stop_summary = gr.Button("‚èπÔ∏è Stop")
        
        audio_status_summary = gr.Textbox(label="üéµ Audio Status", interactive=False)

    with gr.Accordion("üßæ Option 2: Generate Notes from Extracted Text", open=False):
        with gr.Row():
            btn_notes = gr.Button("üìò Generate Study Notes")
        notes_english = gr.Textbox(label="üìë Notes in English", interactive=False)
        language_selector_notes = gr.Dropdown(
            choices=["English", "Hindi", "Tamil", "Telugu", "Kannada", "Malayalam", "Bengali", "Gujarati", "Marathi"],
            label="üåê Translate Notes To",
            value="English"
        )
        notes_translated = gr.Textbox(label="üåç Translated Notes", interactive=False)
        
        # Audio Controls for Notes
        with gr.Row():
            btn_generate_audio_notes = gr.Button("üîä Generate Audio")
            btn_play_notes = gr.Button("‚ñ∂Ô∏è Play")
            btn_pause_notes = gr.Button("‚è∏Ô∏è Pause")
            btn_resume_notes = gr.Button("üîÑ Resume")
            btn_stop_notes = gr.Button("‚èπÔ∏è Stop")
    
        audio_status_notes = gr.Textbox(label="üéµ Audio Status", interactive=False)

    # MODIFIED QUIZ SECTION - Interactive Quiz in Gradio
    with gr.Accordion("üß© Option 3: Interactive Quiz", open=False):
        with gr.Row():
            quiz_num_questions = gr.Dropdown(
                choices=["5", "10"],
                label="üìä Number of Questions",
                value="5"
            )
            btn_generate_quiz = gr.Button("üß© Generate Quiz")
        
        # Quiz Questions (up to 10 questions)
        quiz_components = []
        quiz_radios = []
        
        for i in range(10):
            question_md = gr.Markdown(visible=False)
            question_radio = gr.Radio(visible=False, label="Select your answer:")
            quiz_components.extend([question_md, question_radio])
            quiz_radios.append(question_radio)
        
        btn_submit_quiz = gr.Button("üìù Submit Quiz", visible=False)
        quiz_results = gr.Markdown(visible=False, label="Quiz Results")
    
    with gr.Accordion("üé¨ Option 4: Generate Educational Video", open=False):
        gr.Markdown("### Create an AI-generated educational video with slides and narration")
    
    with gr.Row():
        btn_generate_video = gr.Button("üé¨ Generate Educational Video", variant="primary", size="lg")
    
    video_generation_status = gr.Textbox(
        label="üìπ Video Generation Status", 
        interactive=False,
        placeholder="Click the button above to start generating your educational video..."
    )
    
    # Add video player component
    video_player = gr.Video(
        label="üì∫ Generated Video Player",
        visible=False
    )
    
    video_file_display = gr.File(
        label="üì• Download Video", 
        visible=False,
        file_types=[".mp4"]
    )
    # File Downloads
    download_notes = gr.File(label="üì• Download Notes")
    download_summary = gr.File(label="üì• Download Summary")

    # Button Logic
    btn_scan.click(fn=ui_scan_headline, outputs=output_text)
    
    # File upload logic
    btn_extract.click(fn=ui_upload_and_extract, inputs=file_upload, outputs=output_text)

    btn_watch.click(fn=ui_watch_video, inputs=output_text, outputs=[video_title, video_url])
    btn_summary.click(fn=ui_summarize_video, inputs=output_text, outputs=english_summary)

    # Translation logic
    language_selector_summary.change(
        fn=conditional_translate_summary, 
        inputs=[english_summary, language_selector_summary], 
        outputs=translated_summary
    )

    # Summary Audio Controls
    btn_generate_audio_summary.click(
        fn=ui_generate_audio_summary,
        inputs=[english_summary, translated_summary, language_selector_summary],
        outputs=audio_status_summary
    )
    
    btn_play_summary.click(fn=play_audio, outputs=audio_status_summary)
    btn_pause_summary.click(fn=pause_audio, outputs=audio_status_summary)
    btn_resume_summary.click(fn=resume_audio, outputs=audio_status_summary)
    btn_stop_summary.click(fn=stop_audio, outputs=audio_status_summary)

    # Notes generation and translation
    btn_notes.click(fn=generate_notes_with_gemini, inputs=output_text, outputs=notes_english)
    
    language_selector_notes.change(
        fn=conditional_translate_notes, 
        inputs=[notes_english, language_selector_notes], 
        outputs=notes_translated
    )

    # Notes Audio Controls
    btn_generate_audio_notes.click(
        fn=ui_generate_audio_notes,
        inputs=[notes_english, notes_translated, language_selector_notes],
        outputs=audio_status_notes
    )
    
    btn_play_notes.click(fn=play_audio, outputs=audio_status_notes)
    btn_pause_notes.click(fn=pause_audio, outputs=audio_status_notes)
    btn_resume_notes.click(fn=resume_audio, outputs=audio_status_notes)
    btn_stop_notes.click(fn=stop_audio, outputs=audio_status_notes)
    
    #for video geenration 
    btn_generate_video.click(
        fn=ui_generate_educational_video,
        inputs=output_text,
        outputs=[video_generation_status, video_file_display, video_player]
    )

    # MODIFIED QUIZ BUTTON LOGIC
    def show_quiz_interface(topic, num_questions):
        updates = generate_quiz_interface(topic, num_questions)
        # Show submit button and results area if quiz was generated successfully
        if any(update.get('visible', False) for update in updates if hasattr(update, 'get')):
            updates.extend([
                gr.update(visible=True),   # Submit button
                gr.update(visible=False)   # Results (hidden initially)
            ])
        else:
            updates.extend([
                gr.update(visible=False),  # Submit button
                gr.update(visible=False)   # Results
            ])
        return updates

    btn_generate_quiz.click(
        fn=show_quiz_interface,
        inputs=[output_text, quiz_num_questions],
        outputs=quiz_components + [btn_submit_quiz, quiz_results]
    )

    # Submit quiz logic
    btn_submit_quiz.click(
        fn=lambda *args: [submit_quiz(*args), gr.update(visible=True)],
        inputs=quiz_radios,
        outputs=[quiz_results, quiz_results]
    )

    # File Saving
    notes_english.change(
        fn=lambda text: save_text_to_file(text, "notes.txt"), 
        inputs=notes_english, 
        outputs=download_notes
    )
    english_summary.change(
        fn=lambda text: save_text_to_file(text, "summary.txt"), 
        inputs=english_summary, 
        outputs=download_summary
    )


    demo.launch(share=True)